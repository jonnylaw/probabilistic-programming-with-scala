<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Probabilistic Programming in Scala</title>
<meta name="author" content="(Jonny Law)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="./reveal.js/css/theme/beige.css" id="theme"/>

<link rel="stylesheet" href="./reveal.js/lib/css/tomorrow-night-eighties.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = './reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition=""><h1 class="title">Probabilistic Programming in Scala</h1><h2 class="author">Jonny Law</h2><h2 class="date">2018-12-15 Sat 00:00</h2><p class="date">Created: 2018-12-16 Sun 14:26</p>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-org2f56490">1. Introduction to Bayesian Inference</a></li>
<li><a href="#/slide-org4ad3675">2. Probabilistic Programming</a></li>
<li><a href="#/slide-org4153d94">3. Statistical Computation</a></li>
<li><a href="#/slide-orgacc4de7">4. Functional Programming</a></li>
<li><a href="#/slide-orgf83b69c">5. Category Theory</a></li>
<li><a href="#/slide-org267af4d">6. Automatic Differentiation</a></li>
<li><a href="#/slide-org88c7497">7. Putting it all together</a></li>
<li><a href="#/slide-orgd0e7bec">8. Conclusion</a></li>
</ul>
</div>
</div>
</section>

<section>
<section id="slide-org2f56490" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org2f56490"><span class="section-number-2">1</span> Introduction to Bayesian Inference</h2>
<div class="outline-text-2" id="text-1">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org74c13bb" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org74c13bb"><span class="section-number-3">1.1</span> Bayesian Statistics</h3>
<ul>
<li>Bayesian statistics is concerned with expressing uncertainty using probability</li>
<li>Provides a framework for subjective beliefs and updating - reflecting how
people reason in real life</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org757cccb" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org757cccb"><span class="section-number-3">1.2</span> Bayes Theorem</h3>
<ul>
<li>Bayes theorem allows us to determine the probability of a hypothesis being
true by collecting data related to the hypothesis</li>

</ul>

<p>
\(p(H|y) = \frac{P(y|H)p(H)}{\int p(y)}\)
</p>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgab2201d" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgab2201d"><span class="section-number-3">1.3</span> Finding the integral</h3>
<ul>
<li>The denominator of Bayes theorem is often intractable</li>
<li>Sampling based inference methods can be used to approximate the posterior
distribution</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4ad3675" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org4ad3675"><span class="section-number-2">2</span> Probabilistic Programming</h2>
<div class="outline-text-2" id="text-2">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org71f6377" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org71f6377"><span class="section-number-3">2.1</span> What is Probabilistic Programming?</h3>
<ul>
<li class="fragment appear">Efficiently expressing Bayesian statistical models and performing inference</li>
<li class="fragment appear">Provide a consistent, flexible modelling language for specifying prior
beliefs and likelihooods</li>
<li class="fragment appear">Abstract away the inference algorithms from the user</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgdd9a60c" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgdd9a60c"><span class="section-number-3">2.2</span> Examples of PPLs</h3>
<ul>
<li class="fragment appear">Stan <a href="http://mc-stan.org/">http://mc-stan.org/</a></li>
<li class="fragment appear">BUGS <a href="https://www.mrc-bsu.cam.ac.uk/software/bugs/">https://www.mrc-bsu.cam.ac.uk/software/bugs/</a></li>
<li class="fragment appear">Jags <a href="http://mcmc-jags.sourceforge.net/">http://mcmc-jags.sourceforge.net/</a></li>
<li class="fragment appear">PyMc <a href="https://pymc-devs.github.io/pymc/">https://pymc-devs.github.io/pymc/</a></li>
<li class="fragment appear">Pyro (Uber) <a href="https://github.com/uber/pyro">https://github.com/uber/pyro</a></li>
<li class="fragment appear">TensorFlow Probability (Google) <a href="https://www.tensorflow.org/probability/">https://www.tensorflow.org/probability/</a></li>
<li class="fragment appear">Rainier (Stripe) <a href="https://github.com/stripe/rainier/">https://github.com/stripe/rainier/</a></li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org41dc859" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org41dc859"><span class="section-number-3">2.3</span> Why?</h3>
<ul>
<li class="fragment appear">Small data</li>
<li class="fragment appear">Transparent, interpretable models</li>
<li class="fragment appear">Incorporate expert judgment required in many areas of business</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4153d94" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org4153d94"><span class="section-number-2">3</span> Statistical Computation</h2>
<div class="outline-text-2" id="text-3">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org73aa53b" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org73aa53b"><span class="section-number-3">3.1</span> Conjugacy</h3>
<ul>
<li class="fragment appear">The prior distribution is the same distribution as the posterior
distribution and can be derived analytically</li>
<li class="fragment appear">Only applicable for some models</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org0a4fb3b" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org0a4fb3b"><span class="section-number-3">3.2</span> Gibbs Sampling</h3>
<ul>
<li class="fragment appear">Markov chain Monte Carlo (MCMC) method which exploits conditional probability to derive conditionally conjugate distributions</li>
<li class="fragment appear">Sample from each conditional conjugate distribution in turn to create a
Markov chain representing draws from the full posterior distribution</li>
<li class="fragment appear">Restricts the form of the prior distribution to a conjugate family</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org28f7d5e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org28f7d5e"><span class="section-number-3">3.3</span> Metropolis-Hastings</h3>
<ul>
<li class="fragment appear">General MCMC method with no restriction on prior distributions</li>
<li class="fragment appear">New parameters are proposed from some proposal distribution, \(\psi^\star
     \sim p(\psi^\star|\psi)\) and accepted according to the
Metropolis-Hastings ratio
  \(A = \frac{p(\psi^\star)\pi(Y|\psi^\star)q(\psi|\psi^\star)}{p(\psi)\pi(Y|\psi)q(\psi^\star|\psi)}\)</li>
<li class="fragment appear">\(p(\psi)\) is the prior distribution, \(\pi(Y|\psi)\) is the likelihood,
\(q(\psi|\psi^\star)\) is the probability of moving from \(\psi^\star\) to
\(\psi\)</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgcf2f6a2" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgcf2f6a2"><span class="section-number-3">3.4</span> Metropolis-Hastings</h3>
<div class="org-src-container">

<pre><code class="Scala" >def mhStep[P](posterior: P =&gt; Double, 
              proposal: P =&gt; Dist[P]) = { p: P =&gt;
  for {
    ps &lt;- proposal(p)
    a = posterior(ps) - proposal(p).logPdf(ps) - 
      posterior(p) + proposal(ps).logPdf(p)
    u &lt;- Dist.uniform(0, 1)
    next = if (log(u) &lt; a) ps else p
  } yield next
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgc1dbba2" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgc1dbba2"><span class="section-number-3">3.5</span> Metropolis-Hastings</h3>
<ul>
<li class="fragment appear">Metropolis-Hastings is simple to implement and guaranteed to converge if
it's left long enough - but no one wants to wait forever</li>
<li class="fragment appear">The optimal acceptance ratio is 0.234 - most moves are rejected</li>
<li class="fragment appear">A random-walk proposal centered at the previous parameter is often a
default choice
\(p(\psi^\star|\psi) \sim \mathcal{N}(\psi | \Sigma)\)</li>
<li class="fragment appear">The cost of an independent sample from the stationary distribution is
\(\mathcal{O}(d^2)\) for a \(d\) dimensional parameter space</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org29466be" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org29466be"><span class="section-number-3">3.6</span> Hamiltonian Monte Carlo</h3>
<ul>
<li class="fragment appear">Can we use gradient information from the un-normalised log posterior?</li>
<li class="fragment appear"><p>
Improved proposal based on Hamilton's Equations:
</p>
<div>
\begin{align}
  \frac{\mathrm{d}p}{\mathrm{d}t} &= -\frac{\partial \mathcal{H}}{\partial q}, \\
  \frac{\mathrm{d}q}{\mathrm{d}t} &= +\frac{\partial\mathcal{H}}{\partial p}
\end{align}

</div></li>
<li class="fragment appear">\(\boldsymbol{p}\) is the momentum, equal to \(m\dot{\boldsymbol{q}}\)</li>
<li class="fragment appear">\(\boldsymbol{q}\) is the particle position</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org99ad1dd" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org99ad1dd"><span class="section-number-3">3.7</span> Hamiltonian Monte Carlo</h3>
<ul>
<li class="fragment appear">The static parameters correspond to the position in Hamilton's equations,
the momentum is an auxiliary parameter</li>
<li class="fragment appear">The joint density of the parameters and momentum can be written as:
\(p(\psi, \phi) \propto \exp \left\{ \log p(\psi|y) - \frac{1}{2}\phi^T\phi \right\}\)</li>
<li class="fragment appear">A special discretisation of Hamilton's equations is used which exactly conserves energy called a leapfrog step</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org3162123" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org3162123"><span class="section-number-3">3.8</span> The Leapfrog step</h3>
<div>
\begin{align*}
  \phi_{t+\varepsilon/2} &= \phi_{t-1} + \frac{\varepsilon}{2} \nabla_\psi\log p(y|\psi_{t-1}), \\
  \psi_{t+\varepsilon} &= \psi_{t-1} + \varepsilon \phi_{t+\varepsilon/2}, \\
  \phi_{t+\varepsilon} &= \phi_{t+\varepsilon/2} + \frac{\varepsilon}{2} \nabla\log p(y|\psi_{t+\varepsilon}).
\end{align*}

</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org774cf0e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org774cf0e"><span class="section-number-3">3.9</span> Hamiltonian Monte Carlo</h3>
<ul>
<li class="fragment appear">The leapfrog has a tuning parameter, the step size \(\varepsilon\)</li>
<li class="fragment appear">Only continuous distributions can be used since the un-normalised
log-posterior must be differentiable</li>
<li class="fragment appear">Non conjugate prior distributions can be used, like Metropolis-Hastings</li>
<li class="fragment appear">HMC is more computationally efficient, requiring \(O(d^\frac{5}{4})\) for an
independent sample from the posterior distribution of a \(d\) dimensional
parameter space, the optimal acceptance rate is 0.65</li>
<li class="fragment appear">Calculating derivatives is tedious and error-prone</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orga7a9800" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orga7a9800"><span class="section-number-3">3.10</span> HMC algorithm in Scala</h3>
<div class="org-src-container">

<pre><code class="Scala" >def step(psi: DenseVector[Double]): Rand[DenseVector[Double]] = {
  for {
    phi &lt;- priorPhi
    (propPsi, propPhi) = leapfrogs(eps, gradient, l, psi, phi)
    a = logAcceptance(propPsi, propPhi, psi, phi, ll, priorPhi)
    u &lt;- Uniform(0, 1)
    next = if (log(u) &lt; a) {
      propPsi
    } else {
      psi
    }
  } yield next
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org79e26d7" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org79e26d7"><span class="section-number-3">3.11</span> The Leapfrog step</h3>
<div class="org-src-container">

<pre><code class="Scala" >def leapfrog(
  eps: Double,
  gradient: DenseVector[Double] =&gt; DenseVector[Double])(
  psi: DenseVector[Double],
  phi: DenseVector[Double]) = {
  val p1 = phi + eps * 0.5 * gradient(psi)
  val t1 = psi + eps * p1
  val p2 = p1 + eps * 0.5 * gradient(t1)
  (t1, p2)
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgaea13b1" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgaea13b1"><span class="section-number-3">3.12</span> Multiple leapfrog steps</h3>
<div class="org-src-container">

<pre><code class="Scala" >def leapfrogs(
  eps: Double,
  gradient: DenseVector[Double] =&gt; DenseVector[Double],
  l: Int,
  psi: DenseVector[Double],
  phi: DenseVector[Double]) = {
    if (l == 0) {
      (theta, phi)
    } else {
      val (t, p) = leapfrog(eps, gradient, theta, phi)
      leapfrogs(eps, gradient, l-1, t, p)
    }
  }
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgafb89f2" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgafb89f2"><span class="section-number-3">3.13</span> Tuning Hamiltonian Monte Carlo</h3>
<ul>
<li class="fragment appear">The step size \(\varepsilon\) and the number of leapfrog steps \(l\) are tuning
parameters which can be determined with pilot runs aiming for the optimal
acceptance rate 0.65</li>
<li class="fragment appear">The Dual averaging and the NUTS algorithm can be used to determine an
appropriate step size number of steps</li>
<li class="fragment appear">eHMC is another algorithm for automatically selecting the step size</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgacc4de7" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="orgacc4de7"><span class="section-number-2">4</span> Functional Programming</h2>
<div class="outline-text-2" id="text-4">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgb472204" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgb472204"><span class="section-number-3">4.1</span> Good things</h3>
<ul>
<li class="fragment appear">Pure Functions</li>
<li class="fragment appear">Function Composition</li>
<li class="fragment appear">Immutable Data Structures</li>
<li class="fragment appear">Static Types with type inference</li>
<li class="fragment appear">Predictable, correct programs</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgc79a8b3" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgc79a8b3"><span class="section-number-3">4.2</span> Higher Order Functions</h3>
<ul>
<li>Let's apply a function to a list</li>

</ul>
<div class="org-src-container">

<pre><code class="scala" >val xs = Array(1,2,3,4,5)
var i = 0
while (i < xs.size) {
  xs(i) = xs(i) + 1
  i += 1
}
xs
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="org" >xs: Array[Int] = Array(1, 2, 3, 4, 5)
i: Int = 0
res16: Array[Int] = Array(2, 3, 4, 5, 6)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org4c4067e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org4c4067e"><span class="section-number-4">4.2.1</span> Map</h4>
<ul>
<li>Maps, create a copy of the collection with the updated values</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >xs map (_ + 1)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="org" >res18: Array[Int] = Array(3, 4, 5, 6, 7)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="scala" >def map[A, B](fa: List[A])(f: A => B): List[B]
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org655dadf" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org655dadf"><span class="section-number-4">4.2.2</span> Reduction</h4>
<ul>
<li>Folds, apply a binary operation to a collection using the previous result</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >xs.foldLeft(0)(_ + _)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="org" >res20: Int = 20
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="scala" >def foldLeft[A, B](fa: List[A])(z: B)(f: (B, A) => B): B
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgba2baed" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgba2baed"><span class="section-number-4">4.2.3</span> flatMap</h4>
<ul>
<li>Apply a function which returns a collection, to a collection then flatten
it (sometimes called bind)</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >xs flatMap (x => List(x, x + 1, x + 2))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="org" >res22: Array[Int] = Array(2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7, 6, 7, 8)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="scala" >def flatMap[A, B](fa: List[A])(f: A => List[B]): List[B])
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgefa9e96" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgefa9e96"><span class="section-number-3">4.3</span> Polymorphism</h3>
<ul>
<li class="fragment appear">Sometimes static types are associated with verbosity</li>
<li class="fragment appear">Type inference and ad-hoc Polymorphism can help</li>
<li class="fragment appear">This function will add together all elements in a list which have a numeric type</li>

</ul>
<div class="org-src-container">

<pre><code class="scala" >def sum[A: Numeric](xs: List[A]): A = 
  xs.foldLeft(_ + _)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org67ade43" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org67ade43"><span class="section-number-3">4.4</span> Typeclasses</h3>
<ul>
<li>A typeclass is an abstract implementation of a class</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >trait Numeric[A] {
 def compare(x: T, y: T): Int
 def fromInt(x: Int): T
 def minus(x: T, y: T): T
 def negate(x: T): T
 def plus(x: T, y: T): T
 def times(x: T, y: T): T
 def toDouble(x: T): Double
 def toFloat(x: T): Float
 def toInt(x: T): Int
 def toLong(x: T): Long 
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org6481a6e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org6481a6e"><span class="section-number-3">4.5</span> Typeclasses</h3>
<ul>
<li>Concrete members of a typeclass can be provided using implicit definitions</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >implicit def numericInt = new Numeric[Int] { ... }
</code></pre>
</div>

<ul>
<li>Type safety is retained and we don't have to write functions twice</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgf83b69c" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="orgf83b69c"><span class="section-number-2">5</span> Category Theory</h2>
<div class="outline-text-2" id="text-5">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgcd979b5" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgcd979b5"><span class="section-number-3">5.1</span> What is a Category?</h3>
<ul>
<li class="fragment appear">A category \(\mathcal{C}\) consists of objects \(\textrm{obj}(\mathcal{C})\)
and arrows, or morphisms between categories, \(\textrm{hom}(\mathcal{C})\)</li>
<li class="fragment appear">Morphisms compose, for \(f: X \rightarrow Y\) and \(g: Y \rightarrow Z\), then
\(h: X \rightarrow Z\) is in \(\textrm{hom}(\mathcal{C})\) given by \(g \circ f\)</li>
<li class="fragment appear">Objects must have identity morphisms, written \(\textrm{id}_X: X \rightarrow X\)</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orged380d2" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orged380d2"><span class="section-number-3">5.2</span> Functors</h3>
<ul>
<li class="fragment appear">A functor is a mapping between categories which preserves structure</li>
<li class="fragment appear">\(\mathcal{C}\) and \(\mathcal{D}\) are categories, then a functor \(F:\mathcal{C} \rightarrow \mathcal{D}\): 
<ul>
<li>Associates \(X \in \mathcal{C}\) to an object \(F(X) \in \mathcal{D}\)</li>
<li>And each morphism, \(f:X \rightarrow Y\) in \(\mathcal{C}\) to a morphism in
\(\mathcal{D}\), \(F(f): F(X) \rightarrow F(Y)\).</li>

</ul></li>
<li class="fragment appear">Satisfying
<ul>
<li>\(F(\textrm{id}_X) = \textrm{id}_{F(X)}\) for each \(X \in \mathcal{C}\)</li>
<li>\(F(g \circ f) = F(g) \circ F(f)\) for all morphisms, \(f, g \in \mathcal{C}\)</li>

</ul></li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgb804cfb" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgb804cfb"><span class="section-number-3">5.3</span> Natural Transformation</h3>
<ul>
<li class="fragment appear">A functor is a morphism between two categories, a natural transformation is a morphism between functors</li>
<li class="fragment appear">\(X \in \mathcal{C}\), \(F, G: \mathcal{C} \rightarrow \mathcal{D}\) are functors, then a natural
transformation \(\alpha: F(X) \Rightarrow G(X)\) is a family of morphisms such that:
<ul>
<li>\(\forall X \in \mathcal{C}\) then \(\alpha_X: F(X) \rightarrow G(X)\) is a
morphism in \(\mathcal{D}\)</li>
<li>for each  \(f \in \mathcal{C}\) then \(\alpha_Y \circ F(f) = G(f) \circ \alpha_X\).</li>

</ul></li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-orgcc8df1e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgcc8df1e"><span class="section-number-4">5.3.1</span> Natural Transformation</h4>

<div id="orga0c59ba" class="figure">
<p><img src="./figures/natural_transformation.png" alt="natural_transformation.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Commutative diagram for the natural transformation laws</p>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgf62e8c6" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgf62e8c6"><span class="section-number-3">5.4</span> Monads</h3>
<ul>
<li>A monad is an endofunctor, \(T: \mathcal{C} \rightarrow \mathcal{C}\) with
two natural transformations
<ul>
<li>\(\eta: \textrm{Id}_{\mathcal{c}} \rightarrow T\)</li>
<li>\(\mu: T \circ T \rightarrow T\)</li>

</ul></li>
<li>Such that \(\mu \circ T \mu = \mu \circ \mu T\)</li>
<li>and \(\mu \circ T\eta = \mu \circ \eta T = \textrm{Id}_T\)</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org5bfbce4" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org5bfbce4"><span class="section-number-4">5.4.1</span> Monads</h4>
<table id="org6b4991d" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Commutative diagrams for the monad laws</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left"><img src="./figures/monad_laws.png" alt="monad_laws.png" /></td>
<td class="org-left"><img src="./figures/monad_law_2.png" alt="monad_law_2.png" /></td>
</tr>
</tbody>
</table>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org3c48f98" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org3c48f98"><span class="section-number-3">5.5</span> Why?</h3>
<ul>
<li class="fragment appear">Types and functions form a category, called <code>Hask</code>, every functor is hence
an endofunctor, \(F: \texttt{Hask} \rightarrow \texttt{Hask}\)</li>
<li class="fragment appear">Principled abstractions for functional programming</li>
<li class="fragment appear">Testing mathematical laws instead of individual functions</li>
<li class="fragment appear">Verifying the correctness of programs</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org945f585" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org945f585"><span class="section-number-4">5.5.1</span> Distribution is a Monad</h4>
<ul>
<li>A monad provides a context for an effect</li>
<li>Can preserve immutability be encapsulating random draws in a monad
<code>Rand[A]</code> representing a distribution over the type <code>A</code></li>
<li>Unit is the dirac distribution</li>
<li><code>flatMap</code> represents a joint or marginal distribution</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >def coinFlip(n: Int): Rand[Int] = Beta(3, 3).flatMap(p => Binomial(n, p))
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org3c37e77" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org3c37e77"><span class="section-number-4">5.5.2</span> Syntactic Sugar</h4>
<ul>
<li>For comprehension provides is syntactic sugar for chains of <code>flatMap</code> and
<code>map</code></li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >def coinFlip(n: Int): Rand[Int] = for {
  p <- Beta(3, 3)
} yield Binomial(n, p)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org267af4d" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org267af4d"><span class="section-number-2">6</span> Automatic Differentiation</h2>
<div class="outline-text-2" id="text-6">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org3c549d6" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org3c549d6"><span class="section-number-3">6.1</span> What?</h3>
<ul>
<li class="fragment appear">Calculate the exact derivative of a function at a point</li>
<li class="fragment appear">Not symbolic differentiation</li>
<li class="fragment appear">Not numeric differentiation</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org6cdac9a" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org6cdac9a"><span class="section-number-3">6.2</span> Forward Mode AD</h3>
<ul>
<li class="fragment appear">Consider the function \(f(x) = x^2 + 2x + 5\) with derivative \(f^\prime(x) =
     2x + 2\)</li>
<li class="fragment appear">We wish to calculate the derivative of a \(f\) at a specific value of \(x\),
suppose \(x = 5, f(5) = 40, f^\prime(5) = 12\)</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgfed5104" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgfed5104"><span class="section-number-3">6.3</span> Dual Numbers</h3>
<ul>
<li class="fragment appear"><p>
To perform forward mode AD specify the dual number to \(x = 5\), \(x^\prime =
     5 + \varepsilon\) then calculate \(f(x^\prime)\):
</p>
<div>
\begin{align*}
     f(5 + \varepsilon) &= (5 + \varepsilon)^2 + 2(5 + \varepsilon) + 5 \\
              &= 25 + 10\varepsilon + \varepsilon^2 + 10 + 2\varepsilon + 5 \\
              &= 40 + 12\varepsilon
\end{align*}

</div></li>
<li class="fragment appear">Number of computations depends on the dimension of the input space, ie. the
dimension of the parameters</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgeec1056" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgeec1056"><span class="section-number-3">6.4</span> Implementation</h3>
<ul>
<li>Create a new class representing a dual number:</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >case class Dual(real: Double, eps: Double)
</code></pre>
</div>

<ul>
<li>Define an instance of the <code>Numeric[Dual]</code> typeclass</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org67a055e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org67a055e"><span class="section-number-4">6.4.1</span> Dual Operators</h4>
<div class="org-src-container">

<pre><code class="scala" >def plus(x: Dual, y: Dual) =
  Dual(x.real + y.real, x.eps + y.eps)
def minus(x: Dual, y: Dual): Dual =
  Dual(y.real - x.real, y.eps - x.eps)
def times(x: Dual, y: Dual) =
  Dual(x.real * y.real, x.eps * y.real + y.eps * x.real)
def div(x: Dual, y: Dual) =
  Dual(x.real / y.real, 
  (x.eps * y.real - x.real * y.eps) / (y.real * y.real))
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgd7dd8d7" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgd7dd8d7"><span class="section-number-4">6.4.2</span> Dual Usage</h4>
<div class="org-src-container">

<pre><code class="scala" >def logPos(ys: Vector[Double])(mu: Dual) = 
  (-0.5 * 0.125 * mu * mu) - 0.125 * ys.
    map(_ - mu).
    map(x => x * x).
    reduce(_ + _)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org1aa2bb5" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org1aa2bb5"><span class="section-number-4">6.4.3</span> Extension to Gradients</h4>
<ul>
<li>Define a new Dual class</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >case class DualV(real: Double, dual: Vector[Double])
</code></pre>
</div>

<ul>
<li>The <code>dual</code> argument can be confused between variables, each variable in a
computation must have the same dimension <code>dual</code></li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgffa9e9a" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgffa9e9a"><span class="section-number-3">6.5</span> Reverse Mode AD</h3>
<ul>
<li>Reverse mode AD scales in the dimension of the output space</li>
<li>Faster than forward mode for \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\)
when \(n > m\)</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org88c7497" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org88c7497"><span class="section-number-2">7</span> Putting it all together</h2>
<div class="outline-text-2" id="text-7">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org237056f" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org237056f"><span class="section-number-3">7.1</span> Building our own PPL</h3>
<ul>
<li>Embedded DSL for model building using Monads</li>
<li>Fast generic inference schemes for sampling from posterior distributions</li>
<li>Automatic differentiation for gradient based samplers</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4d573a4" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org4d573a4"><span class="section-number-3">7.2</span> Linear Regression</h3>
<div>
\begin{equation}
y_i = \beta^T x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,
\sigma).
\end{equation}

</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orge4507c6" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orge4507c6"><span class="section-number-4">7.2.1</span> Linear Regression</h4>
<div class="org-src-container">

<pre><code class="Scala" >val model = for {
  b0 &lt;- Normal(0.0, 5.0).param
  b1 &lt;- Normal(0.0, 5.0).param
  b2 &lt;- Normal(0.0, 5.0).param
  sigma &lt;- Gamma(2.0, 2.0).param
  _ &lt;- Predictor.fromDoubleVector { xs =&gt;
    {
      val mean = b0 + b1 * xs.head + b2 * xs(1)
      Normal(mean, sigma)
    }
  }
  .fit(x zip y)
} yield Map("b0" -&gt; b0, "b1" -&gt; b1, "b2" -&gt; b2, "sigma" -&gt; sigma) 

model.sample(EHMC(10), 5000, 10000 * 20, 20)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org47c1784" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org47c1784"><span class="section-number-4">7.2.2</span> Linear Regression</h4>

<div id="org3cb7364" class="figure">
<p><img src="./figures/ehmc_lm.png" alt="ehmc_lm.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Diagnostic plots for simulated data from the linear regression model</p>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgb15cbc3" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgb15cbc3"><span class="section-number-3">7.3</span> Stochastic Volatility</h3>
<ul>
<li>A heteroskedastic time series with latent log-volatility \(\alpha_t\)</li>

</ul>

<div>
\begin{align}
y_t &= \sigma_t\exp\left(\frac{\alpha_t}{2}\right), &\sigma_t &\sim \mathcal{N}(0, 1), \\
\textrm{d}\alpha_t &= \phi(\alpha_t - \mu)\textrm{d}t + \sigma \textrm{d}W_t.
\end{align}

</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org59a87b8" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org59a87b8"><span class="section-number-4">7.3.1</span> Stochastic Volatility</h4>
<div class="org-src-container">

<pre><code class="scala" >val prior = for {
  phi1 <- Beta(5.0, 2.0).param
  phi = 2 * phi1 - 1
  mu <- Normal(0.0, 2.0).param
  sigma <- LogNormal(2.0, 2.0).param
  x0 <- Normal(mu, sigma * sigma / (1 - phi * phi)).param
  t0 = 0.0
} yield (t0, phi, mu, sigma, x0)

def ouStep(phi: Real, mu: Real, sigma: Real, x0: Real, dt: Double) = {
  val mean = mu + (-1.0 * phi * dt).exp * (x0 - mu)
  val variance = sigma.pow(2) * (1 - (-2 * phi * dt).exp) / (2*phi)
  Normal(mean, variance.pow(0.5))
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orga2791b5" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orga2791b5"><span class="section-number-4">7.3.2</span> Stochastic Volatility</h4>
<div class="org-src-container">

<pre><code class="scala" >def step(st: RandomVariable[(Double, Real, Real, Real, Real)],
         y: (Double, Double)) = for {
    (t, phi, mu, sigma, x0) <- st
    dt = y._1 - t
    x1 <- ouStep(phi, mu, sigma, x0, dt).param
    _ <- Normal(0.0, (x1 * 0.5).exp).fit(y._2)
  } yield (t + dt, phi, mu, sigma, x1)


val fullModel = ys.foldLeft(prior)(step)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org06b9d93" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org06b9d93"><span class="section-number-3">7.4</span> Mixture Model</h3>
<ul>
<li>Data is assumed to come from one of \(k\) distributions</li>

</ul>

<div>
\begin{align*}
  y_i &\sim \mathcal{N}(\mu_k, \sigma), \\
  k &\sim \mathcal{F}(\theta).
\end{align*}

</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org4863fda" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org4863fda"><span class="section-number-4">7.4.1</span> Mixture Model</h4>
<div class="org-src-container">

<pre><code class="scala" >val model = for {
  theta1 <- Beta(2.0, 5.0).param
  theta2 <- Beta(2.0, 5.0).param
  alphas = Seq(theta1.log, theta2.log, Real.zero)
  thetas = softmax(alphas)
  mu1 <- Normal(0.0, 5.0).param
  mu2 <- Normal(0.0, 5.0).param
  mu3 <- Normal(0.0, 5.0).param
  mus = Seq(mu1, mu2, mu3)
  sigma <- Gamma(2.0, 10.0).param
  components: Map[Continuous, Real] = mus.zip(thetas).map {
    case (m: Real, t: Real) => (Normal(m, sigma) -> t) }.toMap
  _ <- Mixture(components).fit(ys)
} yield Map("theta1" -> thetas.head, "theta2" -> thetas(1),
              "theta3" -> thetas(2),
                "mu1" -> mu1, "mu2" -> mu2, "mu3" -> mu3, "sigma" -> sigma)

</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgd0e7bec" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="orgd0e7bec"><span class="section-number-2">8</span> Conclusion</h2>
<ul>
<li>Probabilistic programming languages can be used to quickly
prototype new models without implementing and testing new inference schemes</li>
<li>Embedded PPLs can be deployed in production code without rewriting -
reducing bugs and enabling faster inference in industrial settings</li>

</ul>
<div class="slide-footer"></div>
</section>
</section>
</div>
</div>
<script src="./reveal.js/lib/js/head.min.js"></script>
<script src="./reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'slide', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: './reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }]
,});
</script>
</body>
</html>
