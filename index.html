<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Probabilistic Programming in Scala</title>
<meta name="author" content="(Jonny Law)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="./reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="./reveal.js/css/theme/beige.css" id="theme"/>

<link rel="stylesheet" href="./reveal.js/lib/css/tomorrow-night-eighties.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = './reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition=""><h1 class="title">Probabilistic Programming in Scala</h1><h2 class="author">Jonny Law</h2><h2 class="date">2018-12-15 Sat 00:00</h2><p class="date">Created: 2018-12-18 Tue 08:26</p>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-org8b35f10">1. Introduction to Bayesian Inference</a></li>
<li><a href="#/slide-org0c8239f">2. Probabilistic Programming</a></li>
<li><a href="#/slide-org92c01de">3. Statistical Computation</a></li>
<li><a href="#/slide-orgc2b5b55">4. Functional Programming</a></li>
<li><a href="#/slide-orgd8b6fe2">5. Category Theory</a></li>
<li><a href="#/slide-org9e1b2b7">6. Automatic Differentiation</a></li>
<li><a href="#/slide-org9273f60">7. Putting it all together</a></li>
<li><a href="#/slide-org4221d8d">8. Conclusion</a></li>
</ul>
</div>
</div>
</section>

<section>
<section id="slide-org8b35f10" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org8b35f10"><span class="section-number-2">1</span> Introduction to Bayesian Inference</h2>
<div class="outline-text-2" id="text-1">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org9d383ba" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org9d383ba"><span class="section-number-3">1.1</span> Bayesian Statistics</h3>
<ul>
<li>Bayesian statistics is concerned with expressing uncertainty using probability</li>
<li>Provides a framework for subjective beliefs and updating - reflecting how
people reason in real life</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgd1a5c8c" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgd1a5c8c"><span class="section-number-3">1.2</span> Bayes Theorem</h3>
<ul>
<li>Bayes theorem allows us to determine the probability of a hypothesis being
true by collecting data related to the hypothesis</li>

</ul>

<p>
\(p(H|y) = \frac{P(y|H)p(H)}{\int p(y)}\)
</p>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4ea7193" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org4ea7193"><span class="section-number-3">1.3</span> Finding the integral</h3>
<ul>
<li>The denominator of Bayes theorem is often intractable</li>
<li>Sampling based inference methods can be used to approximate the posterior
distribution</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org0c8239f" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org0c8239f"><span class="section-number-2">2</span> Probabilistic Programming</h2>
<div class="outline-text-2" id="text-2">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgb5aeee4" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgb5aeee4"><span class="section-number-3">2.1</span> What is Probabilistic Programming?</h3>
<ul>
<li class="fragment appear">Efficiently expressing Bayesian statistical models and performing inference</li>
<li class="fragment appear">Provide a consistent, flexible modelling language for specifying prior
beliefs and likelihooods</li>
<li class="fragment appear">Abstract away the inference algorithms from the user</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgade28cb" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgade28cb"><span class="section-number-3">2.2</span> Examples of PPLs</h3>
<ul>
<li class="fragment appear">Stan <a href="http://mc-stan.org/">http://mc-stan.org/</a></li>
<li class="fragment appear">BUGS <a href="https://www.mrc-bsu.cam.ac.uk/software/bugs/">https://www.mrc-bsu.cam.ac.uk/software/bugs/</a></li>
<li class="fragment appear">Jags <a href="http://mcmc-jags.sourceforge.net/">http://mcmc-jags.sourceforge.net/</a></li>
<li class="fragment appear">PyMc <a href="https://pymc-devs.github.io/pymc/">https://pymc-devs.github.io/pymc/</a></li>
<li class="fragment appear">Pyro (Uber) <a href="https://github.com/uber/pyro">https://github.com/uber/pyro</a></li>
<li class="fragment appear">TensorFlow Probability (Google) <a href="https://www.tensorflow.org/probability/">https://www.tensorflow.org/probability/</a></li>
<li class="fragment appear">Rainier (Stripe) <a href="https://github.com/stripe/rainier/">https://github.com/stripe/rainier/</a></li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orga6ffd17" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orga6ffd17"><span class="section-number-3">2.3</span> Why?</h3>
<ul>
<li class="fragment appear">Small data</li>
<li class="fragment appear">Transparent, interpretable models</li>
<li class="fragment appear">Incorporate expert judgment required in many areas of business</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org92c01de" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org92c01de"><span class="section-number-2">3</span> Statistical Computation</h2>
<div class="outline-text-2" id="text-3">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orga3cf8c8" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orga3cf8c8"><span class="section-number-3">3.1</span> Conjugacy</h3>
<ul>
<li class="fragment appear">The prior distribution is the same distribution as the posterior
distribution and can be derived analytically</li>
<li class="fragment appear">Only applicable for some models</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org2b76716" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org2b76716"><span class="section-number-3">3.2</span> Gibbs Sampling</h3>
<ul>
<li class="fragment appear">Markov chain Monte Carlo (MCMC) method which exploits conditional probability to derive conditionally conjugate distributions</li>
<li class="fragment appear">Sample from each conditional conjugate distribution in turn to create a
Markov chain representing draws from the full posterior distribution</li>
<li class="fragment appear">Restricts the form of the prior distribution to a conjugate family</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orge527d5e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orge527d5e"><span class="section-number-3">3.3</span> Metropolis-Hastings</h3>
<ul>
<li class="fragment appear">General MCMC method with no restriction on prior distributions</li>
<li class="fragment appear">New parameters are proposed from some proposal distribution, \(\psi^\star
     \sim p(\psi^\star|\psi)\) and accepted according to the
Metropolis-Hastings ratio
  \(A = \frac{p(\psi^\star)\pi(Y|\psi^\star)q(\psi|\psi^\star)}{p(\psi)\pi(Y|\psi)q(\psi^\star|\psi)}\)</li>
<li class="fragment appear">\(p(\psi)\) is the prior distribution, \(\pi(Y|\psi)\) is the likelihood,
\(q(\psi|\psi^\star)\) is the probability of moving from \(\psi^\star\) to
\(\psi\)</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org72a9b96" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org72a9b96"><span class="section-number-4">3.3.1</span> Metropolis-Hastings</h4>
<div class="org-src-container">

<pre><code class="Scala" >def mhStep[P](posterior: P =&gt; Double, 
              proposal: P =&gt; Dist[P]) = { p: P =&gt;
  for {
    ps &lt;- proposal(p)
    a = posterior(ps) - proposal(p).logPdf(ps) - 
      posterior(p) + proposal(ps).logPdf(p)
    u &lt;- Dist.uniform(0, 1)
    next = if (log(u) &lt; a) ps else p
  } yield next
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org654ff41" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org654ff41"><span class="section-number-4">3.3.2</span> Metropolis-Hastings</h4>
<ul>
<li class="fragment appear">Metropolis-Hastings is simple to implement and guaranteed to converge if
it's left long enough - but no one wants to wait forever</li>
<li class="fragment appear">The optimal acceptance ratio is 0.234 - most moves are rejected</li>
<li class="fragment appear">A random-walk proposal centered at the previous parameter is often a
default choice
\(p(\psi^\star|\psi) \sim \mathcal{N}(\psi | \Sigma)\)</li>
<li class="fragment appear">The cost of an independent sample from the stationary distribution is
\(\mathcal{O}(d^2)\) for a \(d\) dimensional parameter space</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org1fa88eb" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org1fa88eb"><span class="section-number-3">3.4</span> Hamiltonian Monte Carlo</h3>
<ul>
<li class="fragment appear">Can we use gradient information from the un-normalised log posterior?</li>
<li class="fragment appear"><p>
Improved proposal based on Hamilton's Equations:
</p>
<div>
\begin{align}
  \frac{\mathrm{d}p}{\mathrm{d}t} &= -\frac{\partial \mathcal{H}}{\partial q}, \\
  \frac{\mathrm{d}q}{\mathrm{d}t} &= +\frac{\partial\mathcal{H}}{\partial p}
\end{align}

</div></li>
<li class="fragment appear">\(\boldsymbol{p}\) is the momentum, equal to \(m\dot{\boldsymbol{q}}\)</li>
<li class="fragment appear">\(\boldsymbol{q}\) is the particle position</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-orgb46f3da" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgb46f3da"><span class="section-number-4">3.4.1</span> Hamiltonian Monte Carlo</h4>
<ul>
<li class="fragment appear">The static parameters correspond to the position in Hamilton's equations,
the momentum is an auxiliary parameter</li>
<li class="fragment appear">The joint density of the parameters and momentum can be written as:
\(p(\psi, \phi) \propto \exp \left\{ \log p(\psi|y) - \frac{1}{2}\phi^T\phi \right\}\)</li>
<li class="fragment appear">A special discretisation of Hamilton's equations is used which exactly conserves energy called a leapfrog step</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org7c82cd9" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org7c82cd9"><span class="section-number-4">3.4.2</span> The Leapfrog step</h4>
<div>
\begin{align*}
  \phi_{t+\varepsilon/2} &= \phi_{t-1} + \frac{\varepsilon}{2} \nabla_\psi\log p(y|\psi_{t-1}), \\
  \psi_{t+\varepsilon} &= \psi_{t-1} + \varepsilon \phi_{t+\varepsilon/2}, \\
  \phi_{t+\varepsilon} &= \phi_{t+\varepsilon/2} + \frac{\varepsilon}{2} \nabla\log p(y|\psi_{t+\varepsilon}).
\end{align*}

</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgd72880b" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgd72880b"><span class="section-number-4">3.4.3</span> Hamiltonian Monte Carlo</h4>
<ul>
<li class="fragment appear">The leapfrog has a tuning parameter, the step size \(\varepsilon\)</li>
<li class="fragment appear">Only continuous distributions can be used since the un-normalised
log-posterior must be differentiable</li>
<li class="fragment appear">Non conjugate prior distributions can be used, like Metropolis-Hastings</li>
<li class="fragment appear">HMC is more computationally efficient, requiring \(O(d^\frac{5}{4})\) for an
independent sample from the posterior distribution of a \(d\) dimensional
parameter space, the optimal acceptance rate is 0.65</li>
<li class="fragment appear">Calculating derivatives is tedious and error-prone</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org3e64b20" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org3e64b20"><span class="section-number-4">3.4.4</span> HMC algorithm in Scala</h4>
<div class="org-src-container">

<pre><code class="Scala" >def step(psi: DenseVector[Double]): Rand[DenseVector[Double]] = {
  for {
    phi &lt;- priorPhi
    (propPsi, propPhi) = leapfrogs(eps, gradient, l, psi, phi)
    a = logAcceptance(propPsi, propPhi, psi, phi, ll, priorPhi)
    u &lt;- Uniform(0, 1)
    next = if (log(u) &lt; a) {
      propPsi
    } else {
      psi
    }
  } yield next
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org241aa1b" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org241aa1b"><span class="section-number-4">3.4.5</span> The Leapfrog step</h4>
<div class="org-src-container">

<pre><code class="Scala" >def leapfrog(
  eps: Double,
  gradient: DenseVector[Double] =&gt; DenseVector[Double])(
  psi: DenseVector[Double],
  phi: DenseVector[Double]) = {
  val p1 = phi + eps * 0.5 * gradient(psi)
  val t1 = psi + eps * p1
  val p2 = p1 + eps * 0.5 * gradient(t1)
  (t1, p2)
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org8ae2397" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org8ae2397"><span class="section-number-4">3.4.6</span> Multiple leapfrog steps</h4>
<div class="org-src-container">

<pre><code class="Scala" >def leapfrogs(
  eps: Double,
  gradient: DenseVector[Double] =&gt; DenseVector[Double],
  l: Int,
  psi: DenseVector[Double],
  phi: DenseVector[Double]) = {
    if (l == 0) {
      (theta, phi)
    } else {
      val (t, p) = leapfrog(eps, gradient, theta, phi)
      leapfrogs(eps, gradient, l-1, t, p)
    }
  }
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org5e42022" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org5e42022"><span class="section-number-3">3.5</span> Tuning Hamiltonian Monte Carlo</h3>
<ul>
<li class="fragment appear">The step size \(\varepsilon\) and the number of leapfrog steps \(l\) are tuning
parameters which can be determined with pilot runs aiming for the optimal
acceptance rate 0.65</li>
<li class="fragment appear">The Dual averaging and the NUTS algorithm can be used to determine an
appropriate step size number of steps</li>
<li class="fragment appear">eHMC is another algorithm for automatically selecting the step size</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgc2b5b55" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="orgc2b5b55"><span class="section-number-2">4</span> Functional Programming</h2>
<div class="outline-text-2" id="text-4">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org9e2b44f" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org9e2b44f"><span class="section-number-3">4.1</span> Good things</h3>
<ul>
<li class="fragment appear">Pure Functions</li>
<li class="fragment appear">Function Composition</li>
<li class="fragment appear">Immutable Data Structures</li>
<li class="fragment appear">Static Types with type inference</li>
<li class="fragment appear">Predictable, correct programs</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4fb87b6" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org4fb87b6"><span class="section-number-3">4.2</span> Higher Order Functions</h3>
<ul>
<li>Let's apply a function to a list</li>

</ul>
<div class="org-src-container">

<pre><code class="scala" >val xs = Array(1,2,3,4,5)
var i = 0
while (i < xs.size) {
  xs(i) = xs(i) + 1
  i += 1
}
xs
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="org" >xs: Array[Int] = Array(1, 2, 3, 4, 5)
i: Int = 0
res16: Array[Int] = Array(2, 3, 4, 5, 6)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgadbe72a" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgadbe72a"><span class="section-number-4">4.2.1</span> Map</h4>
<ul>
<li>Maps, create a copy of the collection with the updated values</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >xs map (_ + 1)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="org" >res18: Array[Int] = Array(3, 4, 5, 6, 7)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="scala" >def map[A, B](fa: List[A])(f: A => B): List[B]
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org99e5c85" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org99e5c85"><span class="section-number-4">4.2.2</span> Reduction</h4>
<ul>
<li>Folds, apply a binary operation to a collection using the previous result</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >xs.foldLeft(0)(_ + _)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="org" >res20: Int = 20
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="scala" >def foldLeft[A, B](fa: List[A])(z: B)(f: (B, A) => B): B
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org07bbbbb" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org07bbbbb"><span class="section-number-4">4.2.3</span> flatMap</h4>
<ul>
<li>Apply a function which returns a collection, to a collection then flatten
it (sometimes called bind)</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >xs flatMap (x => List(x, x + 1, x + 2))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="org" >res22: Array[Int] = Array(2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7, 6, 7, 8)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="scala" >def flatMap[A, B](fa: List[A])(f: A => List[B]): List[B])
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org5b7c7ea" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org5b7c7ea"><span class="section-number-3">4.3</span> Polymorphism</h3>
<ul>
<li class="fragment appear">Sometimes static types are associated with verbosity</li>
<li class="fragment appear">Type inference and ad-hoc Polymorphism can help</li>
<li class="fragment appear">This function will add together all elements in a list which have a numeric type</li>

</ul>
<div class="org-src-container">

<pre><code class="scala" >def sum[A: Numeric](xs: List[A]): A = 
  xs.foldLeft(_ + _)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4567f75" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org4567f75"><span class="section-number-3">4.4</span> Typeclasses</h3>
<ul>
<li>A typeclass is an abstract implementation of a class</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >trait Numeric[A] {
 def compare(x: T, y: T): Int
 def fromInt(x: Int): T
 def minus(x: T, y: T): T
 def negate(x: T): T
 def plus(x: T, y: T): T
 def times(x: T, y: T): T
 def toDouble(x: T): Double
 def toFloat(x: T): Float
 def toInt(x: T): Int
 def toLong(x: T): Long 
}
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org91c50dd" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org91c50dd"><span class="section-number-3">4.5</span> Typeclasses</h3>
<ul>
<li>Concrete members of a typeclass can be provided using implicit definitions</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >implicit def numericInt = new Numeric[Int] { ... }
</code></pre>
</div>

<ul>
<li>Type safety is retained and we don't have to write functions twice</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgd8b6fe2" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="orgd8b6fe2"><span class="section-number-2">5</span> Category Theory</h2>
<div class="outline-text-2" id="text-5">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org7809c28" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org7809c28"><span class="section-number-3">5.1</span> What is a Category?</h3>
<ul>
<li class="fragment appear">A category \(\mathcal{C}\) consists of objects \(\textrm{obj}(\mathcal{C})\)
and arrows, or morphisms between categories, \(\textrm{hom}(\mathcal{C})\)</li>
<li class="fragment appear">Morphisms compose, for \(f: X \rightarrow Y\) and \(g: Y \rightarrow Z\), then
\(h: X \rightarrow Z\) is in \(\textrm{hom}(\mathcal{C})\) given by \(g \circ f\)</li>
<li class="fragment appear">Objects must have identity morphisms, written \(\textrm{id}_X: X \rightarrow X\)</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org8b3e56c" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org8b3e56c"><span class="section-number-3">5.2</span> Functors</h3>
<ul>
<li class="fragment appear">A functor is a mapping between categories which preserves structure</li>
<li class="fragment appear">\(\mathcal{C}\) and \(\mathcal{D}\) are categories, then a functor \(F:\mathcal{C} \rightarrow \mathcal{D}\): 
<ul>
<li>Associates \(X \in \mathcal{C}\) to an object \(F(X) \in \mathcal{D}\)</li>
<li>And each morphism, \(f:X \rightarrow Y\) in \(\mathcal{C}\) to a morphism in
\(\mathcal{D}\), \(F(f): F(X) \rightarrow F(Y)\).</li>

</ul></li>
<li class="fragment appear">Satisfying
<ul>
<li>\(F(\textrm{id}_X) = \textrm{id}_{F(X)}\) for each \(X \in \mathcal{C}\)</li>
<li>\(F(g \circ f) = F(g) \circ F(f)\) for all morphisms, \(f, g \in \mathcal{C}\)</li>

</ul></li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4297c44" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org4297c44"><span class="section-number-3">5.3</span> Natural Transformation</h3>
<ul>
<li class="fragment appear">A functor is a morphism between two categories, a natural transformation is a morphism between functors</li>
<li class="fragment appear">\(X \in \mathcal{C}\), \(F, G: \mathcal{C} \rightarrow \mathcal{D}\) are functors, then a natural
transformation \(\alpha: F(X) \Rightarrow G(X)\) is a family of morphisms such that:
<ul>
<li>\(\forall X \in \mathcal{C}\) then \(\alpha_X: F(X) \rightarrow G(X)\) is a
morphism in \(\mathcal{D}\)</li>
<li>for each  \(f \in \mathcal{C}\) then \(\alpha_Y \circ F(f) = G(f) \circ \alpha_X\).</li>

</ul></li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org5dc4834" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org5dc4834"><span class="section-number-4">5.3.1</span> Natural Transformation</h4>

<div id="org2b016d3" class="figure">
<p><img src="./figures/natural_transformation.png" alt="natural_transformation.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Commutative diagram for the second natural transformation</p>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org3d76648" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org3d76648"><span class="section-number-3">5.4</span> Monads</h3>
<ul>
<li class="fragment appear">A monad is an endofunctor, \(T: \mathcal{C} \rightarrow \mathcal{C}\) with
two natural transformations
<ul>
<li>\(\eta: \textrm{Id}_{\mathcal{c}} \rightarrow T\)</li>
<li>\(\mu: T \circ T \rightarrow T\)</li>

</ul></li>
<li class="fragment appear">Such that \(\mu \circ T \mu = \mu \circ \mu T\)</li>
<li class="fragment appear">and \(\mu \circ T\eta = \mu \circ \eta T = \textrm{Id}_T\)</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org46ba442" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org46ba442"><span class="section-number-4">5.4.1</span> Monads</h4>
<table id="org6243edf" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Commutative diagrams for the monad laws</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left"><img src="./figures/monad_laws.png" alt="monad_laws.png" /></td>
<td class="org-left"><img src="./figures/monad_law_2.png" alt="monad_law_2.png" /></td>
</tr>
</tbody>
</table>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org74f22d4" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org74f22d4"><span class="section-number-3">5.5</span> Why?</h3>
<ul>
<li class="fragment appear">Types and functions form a category, called <code>Hask</code>, every functor is hence
an endofunctor, \(F: \texttt{Hask} \rightarrow \texttt{Hask}\)</li>
<li class="fragment appear">Principled abstractions for functional programming</li>
<li class="fragment appear">Testing mathematical laws instead of individual functions</li>
<li class="fragment appear">Verifying the correctness of programs</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-orge9b9a90" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orge9b9a90"><span class="section-number-4">5.5.1</span> Distribution is a Monad</h4>
<ul>
<li class="fragment appear">A monad provides a context for an effect</li>
<li class="fragment appear">Can preserve immutability be encapsulating random draws in a monad
<code>Rand[A]</code> representing a distribution over the type <code>A</code></li>
<li class="fragment appear">Unit is the dirac distribution</li>
<li class="fragment appear"><code>flatMap</code> represents a joint or marginal distribution</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >def coinFlip(n: Int): Rand[Int] = Beta(3, 3).flatMap(p => Binomial(n, p))
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org9bd7ae3" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org9bd7ae3"><span class="section-number-4">5.5.2</span> Syntactic Sugar</h4>
<ul>
<li>For comprehension provides is syntactic sugar for chains of <code>flatMap</code> and
<code>map</code></li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >def coinFlip(n: Int): Rand[Int] = for {
  p <- Beta(3, 3)
} yield Binomial(n, p)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org9e1b2b7" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org9e1b2b7"><span class="section-number-2">6</span> Automatic Differentiation</h2>
<div class="outline-text-2" id="text-6">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4caafb2" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org4caafb2"><span class="section-number-3">6.1</span> What?</h3>
<ul>
<li class="fragment appear">Calculate the exact derivative of a function at a point</li>
<li class="fragment appear">Not symbolic differentiation</li>
<li class="fragment appear">Not numeric differentiation</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org95618f7" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org95618f7"><span class="section-number-3">6.2</span> Forward Mode AD</h3>
<ul>
<li class="fragment appear">Consider the function \(f(x) = x^2 + 2x + 5\) with derivative \(f^\prime(x) =
     2x + 2\)</li>
<li class="fragment appear">We wish to calculate the derivative of a \(f\) at a specific value of \(x\),
suppose \(x = 5, f(5) = 40, f^\prime(5) = 12\)</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org1f87daa" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org1f87daa"><span class="section-number-3">6.3</span> Dual Numbers</h3>
<ul>
<li class="fragment appear"><p>
To perform forward mode AD specify the dual number to \(x = 5\), \(x^\prime =
     5 + \varepsilon\) then calculate \(f(x^\prime)\):
</p>
<div>
\begin{align*}
     f(5 + \varepsilon) &= (5 + \varepsilon)^2 + 2(5 + \varepsilon) + 5 \\
              &= 25 + 10\varepsilon + \varepsilon^2 + 10 + 2\varepsilon + 5 \\
              &= 40 + 12\varepsilon
\end{align*}

</div></li>
<li class="fragment appear">Number of computations depends on the dimension of the input space, ie. the
dimension of the parameters</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org3ae898b" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org3ae898b"><span class="section-number-3">6.4</span> Implementation</h3>
<ul>
<li>Create a new class representing a dual number:</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >case class Dual(real: Double, eps: Double)
</code></pre>
</div>

<ul>
<li>Define an instance of the <code>Numeric[Dual]</code> typeclass</li>

</ul>

<div class="slide-footer"></div>
</section>
<section id="slide-org5a3bf43" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org5a3bf43"><span class="section-number-4">6.4.1</span> Dual Operators</h4>
<div class="org-src-container">

<pre><code class="scala" >def plus(x: Dual, y: Dual) =
  Dual(x.real + y.real, x.eps + y.eps)
def minus(x: Dual, y: Dual): Dual =
  Dual(y.real - x.real, y.eps - x.eps)
def times(x: Dual, y: Dual) =
  Dual(x.real * y.real, x.eps * y.real + y.eps * x.real)
def div(x: Dual, y: Dual) =
  Dual(x.real / y.real, 
  (x.eps * y.real - x.real * y.eps) / (y.real * y.real))
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgadfd06e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgadfd06e"><span class="section-number-4">6.4.2</span> Dual Usage</h4>
<div class="org-src-container">

<pre><code class="scala" >def logPos(ys: Vector[Double])(mu: Dual) = 
  (-0.5 * 0.125 * mu * mu) - 0.125 * ys.
    map(_ - mu).
    map(x => x * x).
    reduce(_ + _)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgbc43453" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgbc43453"><span class="section-number-4">6.4.3</span> Extension to Gradients</h4>
<ul>
<li>Define a new Dual class</li>

</ul>

<div class="org-src-container">

<pre><code class="scala" >case class DualV(real: Double, dual: Vector[Double])
</code></pre>
</div>

<ul>
<li>The <code>dual</code> argument can be confused between variables, each variable in a
computation must have the same dimension <code>dual</code></li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgc460efc" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgc460efc"><span class="section-number-3">6.5</span> Reverse Mode AD</h3>
<ul>
<li>Reverse mode AD scales in the dimension of the output space</li>
<li>Faster than forward mode for \(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\)
when \(n > m\)</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org9273f60" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org9273f60"><span class="section-number-2">7</span> Putting it all together</h2>
<div class="outline-text-2" id="text-7">
</div>
<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-orgeb0db9d" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="orgeb0db9d"><span class="section-number-3">7.1</span> Building our own PPL</h3>
<ul>
<li>Embedded DSL for model building using Monads</li>
<li>Fast generic inference schemes for sampling from posterior distributions</li>
<li>Automatic differentiation for gradient based samplers</li>

</ul>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org725980a" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org725980a"><span class="section-number-3">7.2</span> Linear Regression</h3>
<div>
\begin{equation}
y_i = \beta^T x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,
\sigma).
\end{equation}

</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgec7e92f" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgec7e92f"><span class="section-number-4">7.2.1</span> Linear Regression</h4>
<div class="org-src-container">

<pre><code class="Scala" >val model = for {
  b0 &lt;- Normal(0.0, 5.0).param
  b1 &lt;- Normal(0.0, 5.0).param
  b2 &lt;- Normal(0.0, 5.0).param
  sigma &lt;- Gamma(2.0, 2.0).param
  _ &lt;- Predictor.fromDoubleVector { xs =&gt;
    {
      val mean = b0 + b1 * xs.head + b2 * xs(1)
      Normal(mean, sigma)
    }
  }
  .fit(x zip y)
} yield Map("b0" -&gt; b0, "b1" -&gt; b1, "b2" -&gt; b2, "sigma" -&gt; sigma) 

model.sample(EHMC(10), 5000, 10000 * 20, 20)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgff14445" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgff14445"><span class="section-number-4">7.2.2</span> Linear Regression</h4>

<div id="org265f9fb" class="figure">
<p><img src="./figures/ehmc_lm.png" alt="ehmc_lm.png" width="400px" />
</p>
<p><span class="figure-number">Figure 2: </span>Diagnostic plots for simulated data from the linear regression model</p>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org846f933" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org846f933"><span class="section-number-3">7.3</span> Stochastic Volatility</h3>
<ul>
<li>A heteroskedastic time series with latent log-volatility \(\alpha_t\)</li>

</ul>

<div>
\begin{align}
y_t &= \sigma_t\exp\left(\frac{\alpha_t}{2}\right), &\sigma_t &\sim \mathcal{N}(0, 1), \\
\textrm{d}\alpha_t &= \phi(\alpha_t - \mu)\textrm{d}t + \sigma \textrm{d}W_t.
\end{align}

</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orga5bd63d" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orga5bd63d"><span class="section-number-4">7.3.1</span> Stochastic Volatility</h4>

<div id="org0b32558" class="figure">
<p><img src="./figures/sv_ou_sims.png" alt="sv_ou_sims.png" width="400px" />
</p>
<p><span class="figure-number">Figure 3: </span>Simulation from a stochastic volatility model with Ornstein-Uhlenbeck latent-state</p>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org6820251" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org6820251"><span class="section-number-4">7.3.2</span> Stochastic Volatility</h4>
<div class="org-src-container">

<pre><code class="scala" >val prior = for {
  phi1 <- Beta(5.0, 2.0).param
  phi = 2 * phi1 - 1
  mu <- Normal(0.0, 2.0).param
  sigma <- LogNormal(2.0, 2.0).param
  x0 <- Normal(mu, sigma * sigma / (1 - phi * phi)).param
  t0 = 0.0
} yield (t0, phi, mu, sigma, x0)

def ouStep(phi: Real, mu: Real, sigma: Real, x0: Real, dt: Double) = {
  val mean = mu + (-1.0 * phi * dt).exp * (x0 - mu)
  val variance = sigma.pow(2) * (1 - (-2 * phi * dt).exp) / (2*phi)
  Normal(mean, variance.pow(0.5))
}
</code></pre>
</div>


<div class="slide-footer"></div>
</section>
<section id="slide-org795f4b2" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org795f4b2"><span class="section-number-4">7.3.3</span> Stochastic Volatility</h4>
<div class="org-src-container">

<pre><code class="scala" >def step(st: RandomVariable[(Double, Real, Real, Real, Real)],
         y: (Double, Double)) = for {
    (t, phi, mu, sigma, x0) <- st
    dt = y._1 - t
    x1 <- ouStep(phi, mu, sigma, x0, dt).param
    _ <- Normal(0.0, (x1 * 0.5).exp).fit(y._2)
  } yield (t + dt, phi, mu, sigma, x1)


val fullModel = ys.foldLeft(prior)(step)
</code></pre>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org8b28f35" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h3 id="org8b28f35"><span class="section-number-3">7.4</span> Mixture Model</h3>
<ul>
<li>Data is assumed to come from one of \(k\) distributions</li>

</ul>

<div>
\begin{align*}
  y_i &\sim \mathcal{N}(\mu_k, \sigma), \\
  k &\sim \mathcal{F}(\theta).
\end{align*}

</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgf5d6455" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgf5d6455"><span class="section-number-4">7.4.1</span> Mixture Model</h4>

<div id="org288f2ea" class="figure">
<p><img src="./figures/mixture_model.png" alt="mixture_model.png" />
</p>
<p><span class="figure-number">Figure 4: </span>500 Simulations from a mixture model with \(\theta = \{0.3, 0.2, 0.5\}\) and \(\mu = \{-2.0, 1.0, 3.0\}\), \(\sigma = 0.5\)</p>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-org3292b6e" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="org3292b6e"><span class="section-number-4">7.4.2</span> Mixture Model</h4>
<div class="org-src-container">

<pre><code class="scala" >val model = for {
  theta1 <- Beta(2.0, 5.0).param
  theta2 <- Beta(2.0, 5.0).param
  alphas = Seq(theta1.log, theta2.log, Real.zero)
  thetas = softmax(alphas)
  mu1 <- Normal(0.0, 5.0).param
  mu2 <- Normal(0.0, 5.0).param
  mu3 <- Normal(0.0, 5.0).param
  mus = Seq(mu1, mu2, mu3)
  sigma <- Gamma(2.0, 10.0).param
  components: Map[Continuous, Real] = mus.zip(thetas).map {
    case (m: Real, t: Real) => (Normal(m, sigma) -> t) }.toMap
  _ <- Mixture(components).fit(ys)
} yield Map("theta1" -> thetas.head, "theta2" -> thetas(1),
              "theta3" -> thetas(2),
                "mu1" -> mu1, "mu2" -> mu2, "mu3" -> mu3, "sigma" -> sigma)

</code></pre>
</div>

<div class="slide-footer"></div>
</section>
<section id="slide-orgd2d75c1" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h4 id="orgd2d75c1"><span class="section-number-4">7.4.3</span> Mixture Model</h4>

<div id="org17f02a6" class="figure">
<p><img src="./figures/mixture_model_posterior.png" alt="mixture_model_posterior.png" width="400px" />
</p>
<p><span class="figure-number">Figure 5: </span>Posterior Densities for the parameters in the mixture model</p>
</div>

<div class="slide-footer"></div>
</section>
</section>
<section>
<section id="slide-org4221d8d" data-background="" data-background-size="" data-background-position="" data-background-repeat="" data-background-transition="">
<div class="slide-header"></div>
<h2 id="org4221d8d"><span class="section-number-2">8</span> Conclusion</h2>
<ul>
<li class="fragment appear">Probabilistic programming languages can be used to quickly
prototype new models without implementing and testing new inference schemes</li>
<li class="fragment appear">Embedded PPLs can be deployed in production code without rewriting -
reducing bugs and enabling faster inference in industrial settings</li>

</ul>
<div class="slide-footer"></div>
</section>
</section>
</div>
</div>
<script src="./reveal.js/lib/js/head.min.js"></script>
<script src="./reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'slide', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: './reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }]
,});
</script>
</body>
</html>
